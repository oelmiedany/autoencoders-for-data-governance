# AUTOGENERATED! DO NOT EDIT! File to edit: ../notebooks/3_training.ipynb.

# %% auto 0
__all__ = ['masked_vae_loss', 'train_variational_autoencoder', 'get_best_model']

# %% ../notebooks/3_training.ipynb 0
from . import autoencoders

import torch
import torch.nn.functional as F

import glob
import os

# %% ../notebooks/3_training.ipynb 2
def masked_vae_loss(reconstruction:torch.Tensor, x:torch.Tensor, not_null_mask:torch.Tensor, binary_mask:torch.Tensor)->tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    '''
    Custom VAE loss function with masking for handling replaced NaN values
    
    Args:
        reconstruction (torch.Tensor): Reconstructed input from the decoder
        x (torch.Tensor): Original input
        mean (torch.Tensor): Mean of the latent distribution
        mask (torch.Tensor): Binary mask where 1 indicates valid values and 0 indicates replaced NaN values

    Returns:
        tuple: (total_loss, reconstruction_loss, kl_divergence_loss)
    '''

    # Reconstruction Loss (masked MSE)
    inverse_binary_mask = ~binary_mask

    binary_mask = not_null_mask & binary_mask[None, :]
    numeric_mask = not_null_mask & inverse_binary_mask[None, :]
    
    mse_loss = F.mse_loss(reconstruction[numeric_mask], x[numeric_mask], reduction='mean')# Only compute MSE for non-masked values
    rmse_loss = torch.sqrt(mse_loss)

    bce_loss = F.binary_cross_entropy(reconstruction[binary_mask], x[binary_mask], reduction='mean')
    
    # Total loss (beta-VAE formulation)
    total_loss = rmse_loss + bce_loss * 2
        
    return total_loss, rmse_loss, bce_loss


# %% ../notebooks/3_training.ipynb 3
def train_variational_autoencoder(model, optimiser, train_loader, validation_loader, binary_mask, n_epochs:int=100, patience:int=10, device:str='cuda'):
    '''
    Training loop for the VAE with early stopping
    
    Args:
        model (VAE): The VAE model
        optimiser (torch.optim.optimiser): The optimiser
        train_loader (DataLoader): Training data
        validation_loader (DataLoader): Validation data
        n_epochs (int): Maximeanm number of epochs
        patience (int): Number of epochs to wait for improvement before stopping
        device (str): Device to train on
    '''
    model = model.to(device)
    binary_mask = binary_mask.to(device)

    first_run = True
    
    best_validation_loss = float('inf')
    patience_counter = 0
    
    for epoch in range(n_epochs):
        model.train()
        train_total_loss = 0
        train_rmse_loss = 0
        train_bce_loss = 0
        
        for data, not_null_mask in train_loader:
            data, not_null_mask = data.to(device), not_null_mask.to(device)
            
            reconstruction, mean = model(data)

            loss, rmse_loss, bce_loss = masked_vae_loss(
                reconstruction, data, not_null_mask, binary_mask
            )
            
            optimiser.zero_grad()
            loss.backward()
            optimiser.step()
            
            train_total_loss = train_total_loss + loss.item()
            train_rmse_loss = train_rmse_loss + rmse_loss.item()
            train_bce_loss = train_bce_loss + bce_loss.item()
            
        # Validation Phase  
        model.eval()
        validation_total_loss = 0
        validation_rmse_loss = 0
        validation_bce_loss = 0
        
        with torch.no_grad():  # No gradients needed for validation
            for data, not_null_mask in validation_loader:
                data, not_null_mask = data.to(device), not_null_mask.to(device)
                
                reconstruction, mean = model(data)
                loss, rmse_loss, bce_loss = masked_vae_loss(
                    reconstruction, data, not_null_mask, binary_mask
                )
                
                validation_total_loss = validation_total_loss + loss.item()
                validation_rmse_loss = validation_rmse_loss + rmse_loss.item()
                validation_bce_loss = validation_bce_loss + bce_loss.item()

        print("Mean stats:", mean.mean().item(), mean.std().item())
        
        # Calculate average losses
        average_train_loss = train_total_loss / len(train_loader)
        average_validation_loss = validation_total_loss / len(validation_loader)
        
        print(f'Epoch {epoch+1}/{n_epochs}:')
        print(f'  Training Loss: {average_train_loss:.4f}')
        print(f'  Total Validation Loss: {average_validation_loss:.4f}, Total RMSE Loss{(validation_rmse_loss/ len(validation_loader)):.4f}, Total BCE Loss{(validation_bce_loss/ len(validation_loader)):.4f}')
        
        # Early stopping check
        if average_validation_loss < best_validation_loss:
            best_validation_loss = average_validation_loss
            patience_counter = 0
            
            if first_run:
                os.makedirs('trained_models', exist_ok=True)
                first_run = False

            file_name = f'trained_models/vae_best-input_size:{model.input_size}.pt'
            torch.save(model.state_dict(), file_name)

        else:
            patience_counter = patience_counter + 1
            if patience_counter >= patience:
                print(f'Early stopping triggered after {epoch+1} epochs')
                break

# %% ../notebooks/3_training.ipynb 4
def get_best_model(model_class, sigmoid_mask, file_name:str = None):
    if file_name is None:
        matching_files = glob.glob('trained_models/vae_best*.pt')

        if matching_files:
            file_name = matching_files[0]
        else:
            raise FileNotFoundError(f'No best model found in ../trained_models/')
    
    parameters = file_name[:]
    parameters = parameters.split('.p')[0]
    parameters = parameters.split('-')[1:]

    model_args = {parameter.split(':')[0] : int(parameter.split(':')[1]) for parameter in parameters}
    model_args['sigmoid_mask'] = sigmoid_mask
 
    model = model_class(**model_args)
    model.load_state_dict(torch.load(file_name, map_location=torch.device('cpu')))

    return model

