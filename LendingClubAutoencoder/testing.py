# AUTOGENERATED! DO NOT EDIT! File to edit: ../notebooks/4_testing.ipynb.

# %% auto 0
__all__ = ['test_error_score', 'test_vae', 'cross_validate_vae']

# %% ../notebooks/4_testing.ipynb 0
from . import preprocessing, autoencoders, training

import torch
import torch.nn.functional as F
from torcheval.metrics.functional import multiclass_f1_score

from datetime import datetime, timedelta

# %% ../notebooks/4_testing.ipynb 2
def test_error_score(reconstruction:torch.Tensor, x:torch.Tensor, not_null_mask:torch.Tensor, binary_mask:torch.Tensor)->tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    '''
    Custom VAE loss function with masking for handling replaced NaN values
    
    Args:
        reconstruction (torch.Tensor): Reconstructed input from the decoder
        x (torch.Tensor): Original input
        mean (torch.Tensor): Mean of the latent distribution
        mask (torch.Tensor): Binary mask where 1 indicates valid values and 0 indicates replaced NaN values

    Returns:
        tuple: (total_loss, reconstruction_loss, kl_divergence_loss)
    '''

    # Reconstruction Loss (masked MSE)
    inverse_binary_mask = ~binary_mask

    binary_mask = not_null_mask & binary_mask[None, :]
    numeric_mask = not_null_mask & inverse_binary_mask[None, :]
    
    mse_loss = F.mse_loss(reconstruction[numeric_mask], x[numeric_mask], reduction='mean')# Only compute MSE for non-masked values
    rmse_loss = torch.sqrt(mse_loss)


    binary_values = (x[binary_mask] > 0.5).long()
    predicted_values = (reconstruction[binary_mask] > 0.5).long()
    f1_score = multiclass_f1_score(predicted_values, binary_values, num_classes=2, average='macro')
        
    return rmse_loss, f1_score


# %% ../notebooks/4_testing.ipynb 3
def test_vae(model_file_name, test_loader, sigmoid_mask, binary_mask, device):
    
    model = training.get_best_model(autoencoders.VariationalAutoencoder, sigmoid_mask, model_file_name)
    
    model.eval()
    test_rmse_loss = 0
    test_f1_score = 0
    n_batches = 0

    with torch.no_grad():
        for data_batch, mask_batch in test_loader:
            data_batch, mask_batch = data_batch.to(device), mask_batch.to(device)
            reconstruction, mean = model(data_batch)
            
            rmse_loss, f1_score = test_error_score(reconstruction, data_batch, mask_batch, binary_mask)
            
            test_rmse_loss = test_rmse_loss + rmse_loss.item()
            test_f1_score = test_f1_score + f1_score.item()
            n_batches = n_batches + 1

    average_rmse_loss = test_rmse_loss / n_batches
    average_f1_score = test_f1_score/ n_batches

    return average_rmse_loss, average_f1_score



# %% ../notebooks/4_testing.ipynb 4
def cross_validate_vae(lending_club_data_handler: preprocessing.DataHandler, start: int, end: int, sigmoid_mask: torch.Tensor, binary_mask: torch.Tensor, learning_rate: float = 1e-3, n_folds: int = 5 ):
    '''
    Perform k-fold cross-validation for the VAE
    
    Args:
        start (datetime): Start date
        end (datetime): End date
        n_folds (int): Number of folds
    '''

    total_days = (end - start).days

    train_size = 0.7
    validation_size = 0.15
    test_size = 0.15

    window_size = total_days / (1+train_size*(n_folds-1))
    train_step_size = window_size * train_size
    validation_step_size = window_size * validation_size
    test_step_size = window_size * test_size

    fold_test_losses = {}

    current_start = start

    for fold in range(n_folds):
        train_start = current_start
        train_end = train_start + timedelta(days=int(train_step_size))

        validation_start = train_end + timedelta(days=1)
        validation_end = validation_start + timedelta(days=int(validation_step_size))

        test_start = validation_end + timedelta(days=1)
        test_end = test_start + timedelta(days=int(test_step_size))

        current_start = validation_start
        
        # Ensure we don't go past the end date
        if test_end > end:
            test_end = end

        train_data, train_mask = lending_club_data_handler.get_train_data(train_start, train_end)
        validation_data, validation_mask = lending_club_data_handler.get_test_data(validation_start, validation_end)
        test_data, test_mask = lending_club_data_handler.get_test_data(test_start, test_end)

        train_loader = preprocessing.to_torch_dataloader(train_data,train_mask)
        validation_loader = preprocessing.to_torch_dataloader(validation_data,validation_mask)
        test_loader = preprocessing.to_torch_dataloader(test_data,test_mask)


        # Instantiate model and optimiser 
        model = autoencoders.VariationalAutoencoder(input_size=len(train_data[0]), sigmoid_mask=sigmoid_mask)
        optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)#original is 1e-3

        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # Train model for this fold 
        training.train_variational_autoencoder(model, optimiser, train_loader, validation_loader, binary_mask=binary_mask, device=device)

        # Evaluate best model for this fold
        model_file_name = f'trained_models/vae_best-input_size:{len(train_data[0])}.pt'
        
        test_rmse_loss, test_f1_score = test_vae(model_file_name, test_loader, device)

        fold_test_losses[fold]={'rmse_loss':test_rmse_loss, 'fq_score':test_f1_score}
        print(f'Test Loss for Fold {fold+1}: {test_rmse_loss:.4f}, {test_f1_score:.4f}')

    return fold_test_losses
