{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp training\n",
    "#|export\n",
    "\n",
    "from LendingClubAutoencoder import autoencoders\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|test\n",
    "from LendingClubAutoencoder import preprocessing\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def masked_vae_loss(reconstruction:torch.Tensor, x:torch.Tensor, mean:torch.Tensor, log_variance:torch.Tensor, mask:torch.Tensor)->tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    '''\n",
    "    Custom VAE loss function with masking for handling replaced NaN values\n",
    "    \n",
    "    Args:\n",
    "        reconstruction (torch.Tensor): Reconstructed input from the decoder\n",
    "        x (torch.Tensor): Original input\n",
    "        mean (torch.Tensor): Mean of the latent distribution\n",
    "        log_variance (torch.Tensor): Log variance of the latent distribution\n",
    "        mask (torch.Tensor): Binary mask where 1 indicates valid values and 0 indicates replaced NaN values\n",
    "\n",
    "    Returns:\n",
    "        tuple: (total_loss, reconstruction_loss, kl_divergence_loss)\n",
    "    '''\n",
    "    # Reconstruction Loss (masked MSE)\n",
    "    \n",
    "    mse_loss = F.mse_loss(reconstruction * mask, x * mask, reduction='sum')# Only compute MSE for non-masked values\n",
    "    mse_loss = mse_loss / mask.sum()# normalise by the number of unmasked elements\n",
    "    rmse_loss = torch.sqrt(mse_loss)\n",
    "    \n",
    "    # KL Divergence Loss\n",
    "    kl_loss = -0.5 * torch.sum(1 + log_variance - mean.pow(2) - log_variance.exp())\n",
    "    kl_loss = kl_loss / x.size(0)  # normalise by batch size\n",
    "    \n",
    "    # Total loss (beta-VAE formulation)\n",
    "    total_loss = rmse_loss + kl_loss\n",
    "        \n",
    "    return total_loss, rmse_loss, kl_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def train_variational_autoencoder(model, optimiser, train_loader, validation_loader, n_epochs:int=100, patience:int=10, device:str='cuda'):\n",
    "    '''\n",
    "    Training loop for the VAE with early stopping\n",
    "    \n",
    "    Args:\n",
    "        model (VAE): The VAE model\n",
    "        optimiser (torch.optim.optimiser): The optimiser\n",
    "        train_loader (DataLoader): Training data\n",
    "        validation_loader (DataLoader): Validation data\n",
    "        n_epochs (int): Maximeanm number of epochs\n",
    "        patience (int): Number of epochs to wait for improvement before stopping\n",
    "        device (str): Device to train on\n",
    "    '''\n",
    "    first_run = True\n",
    "    \n",
    "    best_validation_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_total_loss = 0\n",
    "        train_reconstruction_loss = 0\n",
    "        train_kl_loss = 0\n",
    "        \n",
    "        for data, mask in train_loader:\n",
    "            data, mask = data.to(device), mask.to(device)\n",
    "            \n",
    "            reconstruction, mean, log_variance = model(data)\n",
    "\n",
    "            loss, reconstruction_loss, kl_loss = masked_vae_loss(\n",
    "                reconstruction, data, mean, log_variance, mask\n",
    "            )\n",
    "            \n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            \n",
    "            train_total_loss = train_total_loss + loss.item()\n",
    "            train_reconstruction_loss = train_reconstruction_loss + reconstruction_loss.item()\n",
    "            train_kl_loss = train_kl_loss + kl_loss.item()\n",
    "            \n",
    "        # Validation Phase  \n",
    "        model.eval()\n",
    "        validation_total_loss = 0\n",
    "        validation_reconstruction_loss = 0\n",
    "        validation_kl_loss = 0\n",
    "        \n",
    "        with torch.no_grad():  # No gradients needed for validation\n",
    "            for data, mask in validation_loader:\n",
    "                data, mask = data.to(device), mask.to(device)\n",
    "                \n",
    "                reconstruction, mean, log_variance = model(data)\n",
    "                loss, reconstruction_loss, kl_loss = masked_vae_loss(\n",
    "                    reconstruction, data, mean, log_variance, mask\n",
    "                )\n",
    "                \n",
    "                validation_total_loss = validation_total_loss + loss.item()\n",
    "                validation_reconstruction_loss = validation_reconstruction_loss + reconstruction_loss.item()\n",
    "                validation_kl_loss = validation_kl_loss + kl_loss.item()\n",
    "        \n",
    "        # Calculate average losses\n",
    "        average_train_loss = train_total_loss / len(train_loader)\n",
    "        average_validation_loss = validation_total_loss / len(validation_loader)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{n_epochs}:')\n",
    "        print(f'  Training Loss: {average_train_loss:.4f}')\n",
    "        print(f'  Validation Loss: {average_validation_loss:.4f}')\n",
    "        \n",
    "        # Early stopping check\n",
    "        if average_validation_loss < best_validation_loss:\n",
    "            best_validation_loss = average_validation_loss\n",
    "            patience_counter = 0\n",
    "            \n",
    "            if first_run:\n",
    "                os.makedirs('trained_models', exist_ok=True)\n",
    "                first_run = False\n",
    "\n",
    "            file_name = f'trained_models/vae_best-input_size:{model.input_size}.pt'\n",
    "            torch.save(model.state_dict(), file_name)\n",
    "\n",
    "        else:\n",
    "            patience_counter = patience_counter + 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def get_best_model(model_class, file_name:str = None):\n",
    "    if file_name is None:\n",
    "        matching_files = glob.glob('trained_models/vae_best*.pt')\n",
    "\n",
    "        if matching_files:\n",
    "            file_name = matching_files[0]\n",
    "        else:\n",
    "            raise FileNotFoundError(f'No best model found in ../trained_models/')\n",
    "    \n",
    "    parameters = file_name[:]\n",
    "    parameters = parameters.split('.p')[0]\n",
    "    parameters = parameters.split('-')[1:]\n",
    "\n",
    "    model_args = {parameter.split(':')[0] : int(parameter.split(':')[1]) for parameter in parameters}\n",
    "\n",
    "    model = model_class(**model_args)\n",
    "    model.load_state_dict(torch.load(file_name))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|test\n",
    "\n",
    "#Get and preproces data\n",
    "lending_club_data_handler = preprocessing.DataHandler(csv_path='../local_data/all_lending_club_loan_data_2007-2018.csv')\n",
    "\n",
    "start = datetime(2007,1,1)\n",
    "end = datetime(2008,12,31)\n",
    "train_data, train_mask = lending_club_data_handler.get_train_data(start, end)\n",
    "\n",
    "start = datetime(2009,1,1)\n",
    "end = datetime(2009,12,31)\n",
    "validation_data, validation_mask = lending_club_data_handler.get_test_data(start, end)\n",
    "\n",
    "train_loader = preprocessing.to_torch(train_data, train_mask)\n",
    "validation_loader = preprocessing.to_torch(validation_data, validation_mask)\n",
    "\n",
    "#Train model\n",
    "model = autoencoders.VariationalAutoencoder(input_size=len(train_data[0]), hidden_size=64, latent_size=32)\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=1e-3)#original is 1e-3\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_variational_autoencoder(model, optimiser, train_loader, validation_loader, n_epochs=100, patience=10, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
