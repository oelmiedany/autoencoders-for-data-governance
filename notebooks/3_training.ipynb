{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp training\n",
    "#|export\n",
    "\n",
    "from LendingClubAutoencoder import autoencoders\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|test\n",
    "from LendingClubAutoencoder import preprocessing\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def masked_vae_loss(reconstruction:torch.Tensor, x:torch.Tensor, not_null_mask:torch.Tensor, binary_mask:torch.Tensor)->tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    '''\n",
    "    Custom VAE loss function with masking for handling replaced NaN values\n",
    "    It combines root mean square error for numerical values with the binary cross entropy loss for categorical features \n",
    "    \n",
    "    Args:\n",
    "        reconstruction (torch.Tensor): Reconstructed input from the decoder\n",
    "        x (torch.Tensor): Original input\n",
    "        mean (torch.Tensor): Mean of the latent distribution\n",
    "        mask (torch.Tensor): Binary mask where 1 indicates valid values and 0 indicates replaced NaN values\n",
    "\n",
    "    Returns:\n",
    "        tuple: (total_loss, reconstruction_loss, kl_divergence_loss)\n",
    "    '''\n",
    "\n",
    "    # Create masks for binary and numeric features\n",
    "\n",
    "    inverse_binary_mask = ~binary_mask\n",
    "\n",
    "    binary_mask = not_null_mask & binary_mask[None, :]\n",
    "    numeric_mask = not_null_mask & inverse_binary_mask[None, :]\n",
    "    \n",
    "    # Calculate RMSE loss for numeric features only\n",
    "    mse_loss = F.mse_loss(reconstruction[numeric_mask], x[numeric_mask], reduction='mean')# Only compute MSE for non-masked values\n",
    "    rmse_loss = torch.sqrt(mse_loss)\n",
    "\n",
    "    # Calculate BCE loss for binary features only\n",
    "    bce_loss = F.binary_cross_entropy(reconstruction[binary_mask], x[binary_mask], reduction='mean')\n",
    "    \n",
    "    total_loss = rmse_loss + bce_loss * 2# Combine losses (with weighting for BCE to create parity in the prioritisation of both feature types)\n",
    "        \n",
    "    return total_loss, rmse_loss, bce_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def train_variational_autoencoder(model, optimiser, train_loader, validation_loader, binary_mask, n_epochs:int=100, patience:int=10, device:str='cuda'):\n",
    "    '''\n",
    "    Trains a Variational Autoencoder (VAE) model with early stopping based on validation loss.\n",
    "\n",
    "    Args:\n",
    "        model: The VAE model to be trained.\n",
    "        optimiser: The optimiser for updating model parameters.\n",
    "        train_loader: DataLoader for the training data.\n",
    "        validation_loader: DataLoader for the validation data.\n",
    "        binary_mask: Mask indicating which features are binary.\n",
    "        n_epochs (int, optional): Maximum number of training epochs. Default is 100.\n",
    "        patience (int, optional): Number of epochs to wait for improvement before early stopping. Default is 10.\n",
    "        device (str, optional): Device to use for training ('cuda' or 'cpu'). Default is 'cuda'.\n",
    "\n",
    "    Returns:\n",
    "        None. The best model is saved to disk during training.\n",
    "    '''\n",
    "\n",
    "    # Move model and mask to the specified device\n",
    "    model = model.to(device)\n",
    "    binary_mask = binary_mask.to(device)\n",
    "\n",
    "    first_run = True\n",
    "    \n",
    "    # Initialise early stopping variables\n",
    "    best_validation_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()# Set model to training mode\n",
    "        train_total_loss = 0\n",
    "        train_rmse_loss = 0\n",
    "        train_bce_loss = 0\n",
    "        \n",
    "        for data, not_null_mask in train_loader:\n",
    "            data, not_null_mask = data.to(device), not_null_mask.to(device)\n",
    "            \n",
    "            reconstruction, mean = model(data)\n",
    "\n",
    "            loss, rmse_loss, bce_loss = masked_vae_loss(\n",
    "                reconstruction, data, not_null_mask, binary_mask\n",
    "            )# Forward pass and compute loss\n",
    "            \n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()#Backpropagation\n",
    "            optimiser.step()#Optimiser step\n",
    "            \n",
    "            train_total_loss = train_total_loss + loss.item()# Accumulate training losses\n",
    "            train_rmse_loss = train_rmse_loss + rmse_loss.item()\n",
    "            train_bce_loss = train_bce_loss + bce_loss.item()\n",
    "            \n",
    "        # Validation Phase  \n",
    "        model.eval()# Set model to evaluation mode\n",
    "        validation_total_loss = 0\n",
    "        validation_rmse_loss = 0\n",
    "        validation_bce_loss = 0\n",
    "        \n",
    "        with torch.no_grad():  # No gradients needed for validation\n",
    "            for data, not_null_mask in validation_loader:\n",
    "                data, not_null_mask = data.to(device), not_null_mask.to(device)\n",
    "                \n",
    "                reconstruction, mean = model(data)\n",
    "                loss, rmse_loss, bce_loss = masked_vae_loss(\n",
    "                    reconstruction, data, not_null_mask, binary_mask\n",
    "                )\n",
    "                \n",
    "                validation_total_loss = validation_total_loss + loss.item()# Accumulate validation losses\n",
    "                validation_rmse_loss = validation_rmse_loss + rmse_loss.item()\n",
    "                validation_bce_loss = validation_bce_loss + bce_loss.item()\n",
    "\n",
    "        print(\"Mean stats:\", mean.mean().item(), mean.std().item())# Print statistics for the current epoch\n",
    "        \n",
    "        # Calculate average losses\n",
    "        average_train_loss = train_total_loss / len(train_loader)\n",
    "        average_validation_loss = validation_total_loss / len(validation_loader)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{n_epochs}:')\n",
    "        print(f'  Training Loss: {average_train_loss:.4f}')\n",
    "        print(f'  Total Validation Loss: {average_validation_loss:.4f}, Total RMSE Loss{(validation_rmse_loss/ len(validation_loader)):.4f}, Total BCE Loss{(validation_bce_loss/ len(validation_loader)):.4f}')\n",
    "        \n",
    "        # Early stopping check\n",
    "        if average_validation_loss < best_validation_loss:\n",
    "            best_validation_loss = average_validation_loss\n",
    "            patience_counter = 0\n",
    "            \n",
    "            if first_run:\n",
    "                os.makedirs('trained_models', exist_ok=True)\n",
    "                first_run = False\n",
    "\n",
    "            file_name = f'trained_models/vae_best-input_size:{model.input_size}.pt'\n",
    "            torch.save(model.state_dict(), file_name)\n",
    "\n",
    "        else:\n",
    "            patience_counter = patience_counter + 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def get_best_model(model_class, sigmoid_mask, file_name:str = None):\n",
    "    '''\n",
    "    Loads the best saved VAE model from disk, reconstructing its arguments from the filename.\n",
    "\n",
    "    Args:\n",
    "        model_class: The class of the VAE model to instantiate.\n",
    "        sigmoid_mask: Mask indicating which features require a sigmoid activation.\n",
    "        file_name (str, optional): Path to the saved model file. If None, will search for the best model in 'trained_models/'.\n",
    "\n",
    "    Returns:\n",
    "        model: The loaded VAE model with weights restored.\n",
    "    '''\n",
    "    if file_name is None:\n",
    "        matching_files = glob.glob('trained_models/vae_best*.pt')#Returns first eligible model if specific model is not outlined\n",
    "\n",
    "        if matching_files:\n",
    "            file_name = matching_files[0]\n",
    "        else:\n",
    "            raise FileNotFoundError(f'No best model found in ../trained_models/')\n",
    "    \n",
    "    # Parse model parameters from the filename\n",
    "    parameters = file_name[:]\n",
    "    parameters = parameters.split('.p')[0]\n",
    "    parameters = parameters.split('-')[1:]\n",
    "\n",
    "    model_args = {parameter.split(':')[0] : int(parameter.split(':')[1]) for parameter in parameters}\n",
    "    model_args['sigmoid_mask'] = sigmoid_mask\n",
    " \n",
    "    model = model_class(**model_args)\n",
    "    model.load_state_dict(torch.load(file_name, map_location=torch.device('cpu')))# Load model weights from file\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|test\n",
    "\n",
    "#Get and preproces data\n",
    "lending_club_data_handler = preprocessing.DataHandler(csv_path='../local_data/all_lending_club_loan_data_2007-2018.csv')\n",
    "\n",
    "start = datetime(2007,1,1)\n",
    "end = datetime(2008,12,31)\n",
    "train_data, train_mask = lending_club_data_handler.get_train_data(start, end)\n",
    "\n",
    "start = datetime(2009,1,1)\n",
    "end = datetime(2009,12,31)\n",
    "validation_data, validation_mask = lending_club_data_handler.get_test_data(start, end)\n",
    "\n",
    "train_loader = preprocessing.to_torch_dataloader(train_data, train_mask)\n",
    "validation_loader = preprocessing.to_torch_dataloader(validation_data, validation_mask)\n",
    "\n",
    "#Train model\n",
    "model = autoencoders.VariationalAutoencoder(input_size=len(train_data[0]), hidden_size=64, latent_size=32)\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=1e-3)#original is 1e-3\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_variational_autoencoder(model, optimiser, train_loader, validation_loader, n_epochs=100, patience=10, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
