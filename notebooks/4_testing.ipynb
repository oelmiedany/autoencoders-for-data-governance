{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp testing\n",
    "#|export\n",
    "\n",
    "from LendingClubAutoencoder import preprocessing, autoencoders, training\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torcheval.metrics.functional import multiclass_f1_score\n",
    "\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|test\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def test_error_score(reconstruction:torch.Tensor, x:torch.Tensor, not_null_mask:torch.Tensor, binary_mask:torch.Tensor)->tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    '''\n",
    "    Calculates the RMSE for numeric features and F1 score for binary features in the VAE reconstruction, using masks to ignore missing values.\n",
    "\n",
    "    Args:\n",
    "        reconstruction (torch.Tensor): The reconstructed input from the VAE decoder.\n",
    "        x (torch.Tensor): The original input tensor.\n",
    "        not_null_mask (torch.Tensor): Mask indicating non-missing values in the input.\n",
    "        binary_mask (torch.Tensor): Mask indicating which features are binary.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (rmse_loss, f1_score) for numeric and binary features respectively.\n",
    "    '''\n",
    "    inverse_binary_mask = ~binary_mask\n",
    "\n",
    "    binary_mask = not_null_mask & binary_mask[None, :]# Calculate which features are numeric and which are binary\n",
    "    numeric_mask = not_null_mask & inverse_binary_mask[None, :]\n",
    "    \n",
    "    # RMSE\n",
    "    mse_loss = F.mse_loss(reconstruction[numeric_mask], x[numeric_mask], reduction='mean')# Only compute MSE for non-masked values\n",
    "    rmse_loss = torch.sqrt(mse_loss)\n",
    "\n",
    "    # F1 Score\n",
    "    binary_values = (x[binary_mask] > 0.5).long()\n",
    "    predicted_values = (reconstruction[binary_mask] > 0.5).long()\n",
    "    f1_score = multiclass_f1_score(predicted_values, binary_values, num_classes=2, average='macro')\n",
    "        \n",
    "    return rmse_loss, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def test_vae(model_file_name, test_loader, sigmoid_mask, binary_mask, device):\n",
    "    '''\n",
    "    Evaluates a trained VAE model on a test set, returning average RMSE and F1 score.\n",
    "\n",
    "    Args:\n",
    "        model_file_name (str): Path to the saved model file.\n",
    "        test_loader (DataLoader): DataLoader for the test set.\n",
    "        sigmoid_mask (torch.Tensor): Mask for features to apply sigmoid activation.\n",
    "        binary_mask (torch.Tensor): Mask for binary features.\n",
    "        device (torch.device): Device to run the evaluation on.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (average_rmse_loss, average_f1_score) across all test batches.\n",
    "    '''\n",
    "    model = training.get_best_model(autoencoders.VariationalAutoencoder, sigmoid_mask, model_file_name)#Loads model\n",
    "    \n",
    "    model.eval()# Sets model into evaluation mode\n",
    "    test_rmse_loss = 0\n",
    "    test_f1_score = 0\n",
    "    n_batches = 0\n",
    "\n",
    "    #Test loop\n",
    "    with torch.no_grad():\n",
    "        for data_batch, mask_batch in test_loader:\n",
    "            data_batch, mask_batch = data_batch.to(device), mask_batch.to(device)\n",
    "            reconstruction, mean = model(data_batch)\n",
    "            \n",
    "            rmse_loss, f1_score = test_error_score(reconstruction, data_batch, mask_batch, binary_mask)\n",
    "            \n",
    "            test_rmse_loss = test_rmse_loss + rmse_loss.item()\n",
    "            test_f1_score = test_f1_score + f1_score.item()\n",
    "            n_batches = n_batches + 1\n",
    "\n",
    "    average_rmse_loss = test_rmse_loss / n_batches\n",
    "    average_f1_score = test_f1_score/ n_batches\n",
    "\n",
    "    return average_rmse_loss, average_f1_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def cross_validate_vae(lending_club_data_handler: preprocessing.DataHandler, start: int, end: int, sigmoid_mask: torch.Tensor, binary_mask: torch.Tensor, learning_rate: float = 1e-3, n_folds: int = 5 ):\n",
    "    '''\n",
    "    Performs k-fold cross-validation for the VAE model using a time-based sliding window.\n",
    "\n",
    "    Args:\n",
    "        lending_club_data_handler (DataHandler): Handler for loading and masking data.\n",
    "        start (datetime): Start date for the data window.\n",
    "        end (datetime): End date for the data window.\n",
    "        sigmoid_mask (torch.Tensor): Mask for features to apply sigmoid activation.\n",
    "        binary_mask (torch.Tensor): Mask for binary features.\n",
    "        learning_rate (float, optional): Learning rate for the optimiser. Defaults to 1e-3.\n",
    "        n_folds (int, optional): Number of cross-validation folds. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        dict: Test losses and F1 scores for each fold.\n",
    "    '''\n",
    "\n",
    "    total_days = (end - start).days\n",
    "\n",
    "    # Outlines propotion of each dataset\n",
    "    train_size = 0.7\n",
    "    validation_size = 0.15\n",
    "    test_size = 0.15\n",
    "\n",
    "    # Calculate window sizes for train, validation, and test splits\n",
    "    window_size = total_days / (1+train_size*(n_folds-1))\n",
    "    train_step_size = window_size * train_size\n",
    "    validation_step_size = window_size * validation_size\n",
    "    test_step_size = window_size * test_size\n",
    "\n",
    "    fold_test_losses = {}\n",
    "\n",
    "    current_start = start\n",
    "\n",
    "    # Loops over each fold\n",
    "    for fold in range(n_folds):\n",
    "        # Defines train, validation, and test periods for this fold\n",
    "        train_start = current_start\n",
    "        train_end = train_start + timedelta(days=int(train_step_size))\n",
    "\n",
    "        validation_start = train_end + timedelta(days=1)\n",
    "        validation_end = validation_start + timedelta(days=int(validation_step_size))\n",
    "\n",
    "        test_start = validation_end + timedelta(days=1)\n",
    "        test_end = test_start + timedelta(days=int(test_step_size))\n",
    "\n",
    "        current_start = validation_start\n",
    "        \n",
    "        # Ensure we don't go past the end date\n",
    "        if test_end > end:\n",
    "            test_end = end\n",
    "\n",
    "        train_data, train_mask = lending_club_data_handler.get_train_data(train_start, train_end)\n",
    "        validation_data, validation_mask = lending_club_data_handler.get_test_data(validation_start, validation_end)\n",
    "        test_data, test_mask = lending_club_data_handler.get_test_data(test_start, test_end)\n",
    "\n",
    "        train_loader = preprocessing.to_torch_dataloader(train_data,train_mask)\n",
    "        validation_loader = preprocessing.to_torch_dataloader(validation_data,validation_mask)\n",
    "        test_loader = preprocessing.to_torch_dataloader(test_data,test_mask)\n",
    "\n",
    "        # Instantiate model and optimiser \n",
    "        model = autoencoders.VariationalAutoencoder(input_size=len(train_data[0]), sigmoid_mask=sigmoid_mask)\n",
    "        optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)#original is 1e-3\n",
    "\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Train model for this fold \n",
    "        training.train_variational_autoencoder(model, optimiser, train_loader, validation_loader, binary_mask=binary_mask, device=device)\n",
    "\n",
    "        # Evaluate best model for this fold\n",
    "        model_file_name = f'trained_models/vae_best-input_size:{len(train_data[0])}.pt'\n",
    "        \n",
    "        test_rmse_loss, test_f1_score = test_vae(model_file_name, test_loader, device)\n",
    "\n",
    "        fold_test_losses[fold]={'rmse_loss':test_rmse_loss, 'fq_score':test_f1_score}\n",
    "        print(f'Test Loss for Fold {fold+1}: {test_rmse_loss:.4f}, {test_f1_score:.4f}')\n",
    "\n",
    "    return fold_test_losses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
