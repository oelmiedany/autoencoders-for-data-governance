{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp preprocessing\n",
    "#|export\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class DataHandler():\n",
    "    '''\n",
    "    Handles preprocessing, cleaning, and transformation of Lending Club data for downstream machine learning task.\n",
    "    It provides methods for extracting, transforming, and batching data for model training and evaluation.\n",
    "    '''\n",
    "    def __init__(self, csv_path: str = '../local_data/all_lending_club_loan_data_2007-2018.csv'):\n",
    "        '''\n",
    "        Initialises the DataHandler.\n",
    "\n",
    "        - Sets up file paths for the cleaned CSV and feature metadata, and ensures the data is preprocessed and ready for use.\n",
    "        - If the cleaned data does not exist, it will trigger the cleaning process.\n",
    "        - Loads feature metadata and initialises the column transformer for feature scaling and encoding.\n",
    "\n",
    "        Args:\n",
    "            csv_path (str): Path to the raw Lending Club CSV file.\n",
    "        '''\n",
    "\n",
    "        self.cleaned_csv_path = f'{csv_path[:-4]}_cleaned.csv'#Path to cleaned data\n",
    "        self.features_path = f'{csv_path[:-4]}_cleaned_features.json'#Path to json listing the features in the cleaned dataset, organised by their type\n",
    "\n",
    "        #If a 'cleaned' version of the data is not found the raw Lending Club data from Kaggle then:\n",
    "        if not os.path.exists(self.cleaned_csv_path):\n",
    "            self.strip_non_data_rows_from_lending_club_data(csv_path)#1. Formatting irregularities relating to header and footer rows are removed\n",
    "            self.clean_lending_club_data()#2.  All variables are processed based on their data type\n",
    "\n",
    "        #If the clean data is found the data features loaded to initialise the column transformer\n",
    "        with open(self.features_path, 'r') as f:\n",
    "            self.features = json.load(f)\n",
    "\n",
    "        self.transformer = self.column_transformer = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('standard_scaler', StandardScaler(), self.features['standard_scaler']),#Numerical data is standard scaled\n",
    "                ('min_max_scaler', MinMaxScaler(), self.features['min_max_scaler']),#Ordinal data is scaled using their min and max to maintain the distance between each option \n",
    "                ('one_hot_encoder', OneHotEncoder(drop=None, handle_unknown='ignore'), self.features['categorical'] )#Categorical data is one hot encoded to avoid creating untrue patterns between data points\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        #Initialises key values and flags if the training data has been set or not based on whether the values are None\n",
    "        self.training_data_start_date = None\n",
    "        self.training_data_end_date = None\n",
    "    \n",
    "    def strip_non_data_rows_from_lending_club_data(self, csv_path: str):\n",
    "        '''\n",
    "        Removes non-data rows from the raw Lending Club CSV file.\n",
    "\n",
    "        The Lending Club data may contain header or footer rows that are not part of the actual dataset.\n",
    "        This function filters out such rows, keeping only those that start with a digit or the header row, and writes the result to a temporary file.\n",
    "\n",
    "        Args:\n",
    "            csv_path (str): Path to the raw Lending Club CSV file.\n",
    "        '''\n",
    "        with open(csv_path, 'r') as f:\n",
    "            lines = [line for line in f if line[0].isdigit() or line.startswith('id')]\n",
    "\n",
    "        with open('temp.csv', 'w') as f:\n",
    "            f.writelines(lines)#Stores the treated data in a temporary file ready to be processed\n",
    "\n",
    "    def is_date_value(self, value: str) -> bool:\n",
    "        '''\n",
    "        Checks if a string value matches the month-year date format (e.g., 'sep-2015').\n",
    "        It's used to identify columns that contain date information in the Lending Club dataset.\n",
    "\n",
    "        Args:\n",
    "            value (str): The value to check.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the value matches the expected date format, False otherwise.\n",
    "        '''\n",
    "        if not isinstance(value, str) or '-' not in value:\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            month_str, year_str = value.lower().split('-')\n",
    "            return (month_str.title() in calendar.month_abbr and \n",
    "                    year_str.isdigit() and \n",
    "                    len(year_str) == 4)\n",
    "        except:\n",
    "            return False\n",
    "        \n",
    "    def parse_date_value(self, value: str) -> datetime:\n",
    "        '''\n",
    "        Converts a month-year string (e.g., 'sep-2015') to a datetime object.\n",
    "\n",
    "        Args:\n",
    "            value (str): The string to convert.\n",
    "\n",
    "        Returns:\n",
    "            datetime: The corresponding datetime object, or None if conversion fails.\n",
    "        '''\n",
    "        if not isinstance(value, str):\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            month_str, year_str = value.lower().split('-')\n",
    "            month_num = list(calendar.month_abbr).index(month_str.title())\n",
    "            return datetime(int(year_str), month_num, 1)\n",
    "        except:\n",
    "            return None\n",
    "            \n",
    "    def drop_undesired_columns(self, df: pl.DataFrame)-> pl.DataFrame:\n",
    "        '''\n",
    "        Removes columns that are redundant, mostly null, or not useful for modelling.\n",
    "\n",
    "        This includes explicit columns known to be unhelpful, columns with high null rates, and columns with certain prefixes.\n",
    "\n",
    "        Args:\n",
    "            df (pl.DataFrame): The input Polars DataFrame.\n",
    "\n",
    "        Returns:\n",
    "            pl.DataFrame: The DataFrame with undesired columns dropped.\n",
    "        '''\n",
    "        explicit_columns_to_drop = [\n",
    "            'id',                       # Unique identifier\n",
    "            'funded_amnt',              # Redundant due to loan_amnt\n",
    "            'funded_amnt_inv',          # Redundant due to loan_amnt\n",
    "            'sub_grade',                # Redundant due to grade column\n",
    "            'emp_title',                # Too random\n",
    "            'title',                    # Redundant in relation to purpose column\n",
    "            'desc',                     # Mostly null\n",
    "            'url',                      # No predictive value\n",
    "            'mths_since_last_delinq',   # Mostly null and redundant due to delinq_2yrs column\n",
    "            'mths_since_last_record',   # Mostly null\n",
    "            'pymnt_plan',               # Always 'n'\n",
    "            'addr_state',               # Reduce dimensionality of this excerise\n",
    "            'zip_code'                  # Reduce dimensionality of this excerise\n",
    "        ]\n",
    "\n",
    "        # Dropping all secondary applicant, hardship, settlement, and joint columns as they are all null\n",
    "        implicit_columns_to_drop = (\n",
    "            'sec_app_',\n",
    "            'hardship_',\n",
    "            'settlement_',\n",
    "            'joint_',\n",
    "        )\n",
    "\n",
    "        implicit_columns_to_drop = [column for column in df.columns if column.startswith(implicit_columns_to_drop)]\n",
    "        high_null_cols = [column for column in df.columns  if (df[column].null_count() / len(df)) > 0.2]#If  more than 20% of the values are null the column is dropped\n",
    "\n",
    "        columns_to_drop = explicit_columns_to_drop + implicit_columns_to_drop + high_null_cols\n",
    "        columns_to_drop = list(set(columns_to_drop))\n",
    "        \n",
    "        return df.drop(columns_to_drop)\n",
    "\n",
    "    def convert_employment_length(self, value: str) -> float:\n",
    "        '''\n",
    "        Converts employment length from string format to a float.\n",
    "        It handles special cases such as '< 1 year' and '10+ years', and returns None for missing values.\n",
    "\n",
    "        Args:\n",
    "            value (str): The employment length as a string.\n",
    "\n",
    "        Returns:\n",
    "            float: The employment length in years, or None if not applicable.\n",
    "        '''\n",
    "\n",
    "        if value is None or value == 'n/a':\n",
    "            return None\n",
    "        if value == '< 1 year':\n",
    "            return 0.5\n",
    "        if value == '10+ years':\n",
    "            return 10.0\n",
    "        return float(value.split()[0])\n",
    "\n",
    "    def clean_lending_club_data(self):\n",
    "        '''\n",
    "        Clean and preprocess the Lending Club dataset.\n",
    "\n",
    "        - Identifies and removes columns that are entirely null or contain date strings.\n",
    "        - Converts date columns to datetime objects and adds derived features.\n",
    "        - Converts and engineers other features (e.g., employment length, grade).\n",
    "        - Drops undesired columns.\n",
    "        - Saves the cleaned data and feature metadata for later use.\n",
    "        '''\n",
    "\n",
    "        df = pl.read_csv('temp.csv')\n",
    "\n",
    "        null_columns = []\n",
    "        date_columns = []\n",
    "        \n",
    "        #Identify null and date columns\n",
    "        for col in df.columns:\n",
    "            # Check if column is all null\n",
    "            if df[col].is_null().all():\n",
    "                null_columns.append(col)\n",
    "                continue\n",
    "            \n",
    "            #Identify date columns\n",
    "            sample = df[col].drop_nulls().sample(1)\n",
    "            if self.is_date_value(str(sample[0])):\n",
    "                date_columns.append(col)\n",
    "\n",
    "        df = df.drop(null_columns)\n",
    "\n",
    "        #Convert date columns to datetime\n",
    "        if date_columns:\n",
    "            df = df.with_columns([\n",
    "                pl.col(col)\n",
    "                .str.to_lowercase()\n",
    "                .map_elements(self.parse_date_value, return_dtype=datetime)\n",
    "                .alias(col)\n",
    "                for col in date_columns\n",
    "            ])\n",
    "\n",
    "            #Include month and year columns for each date column\n",
    "            expressions = []\n",
    "            for col in date_columns:\n",
    "                expressions.extend([\n",
    "                    (pl.col(col).cast(pl.Int64)/pl.lit(2.628e+15)).alias(f'{col}_unicode_month')\n",
    "                ])\n",
    "            df = df.with_columns(expressions)\n",
    "\n",
    "        #Ensures ordinal data is stored as integers while numerical data are floats\n",
    "        df = df.with_columns([\n",
    "            pl.col('term').str.extract(r'(\\d+)').cast(pl.Int64).alias('term_months'),\n",
    "            pl.col('emp_length').map_elements(self.convert_employment_length, return_dtype=pl.Float64).alias('employment_years'),\n",
    "            pl.col('grade').str.to_uppercase().map_elements(lambda x: ord(x) - 64, return_dtype=pl.Int64).alias('grade'),\n",
    "            pl.col('debt_settlement_flag').map_elements(lambda x: 1 if x == 'Y' else 0, return_dtype=pl.Int64).alias('debt_settlement_flag'),\n",
    "            pl.col('orig_projected_additional_accrued_interest').cast(pl.Float64).alias('orig_projected_additional_accrued_interest')\n",
    "            ]).drop(['emp_length', 'term'])\n",
    "        \n",
    "        df = self.drop_undesired_columns(df)\n",
    "\n",
    "        os.remove('temp.csv')#Removes temporary file\n",
    "\n",
    "        #Seperates the different columns by their data type\n",
    "        standard_scaler_columns = [column for column, dtype in df.schema.items() \n",
    "                           if dtype==pl.Float64]\n",
    "        min_max_scaler_columns = [column for column, dtype in df.schema.items() \n",
    "                           if dtype==pl.Int64]\n",
    "        categorical_columns = [col for col, dtype in df.schema.items() \n",
    "                               if dtype == pl.Utf8]\n",
    "\n",
    "        features_dict = {\n",
    "            'standard_scaler': standard_scaler_columns,\n",
    "            'min_max_scaler': min_max_scaler_columns,\n",
    "            'categorical': categorical_columns\n",
    "        }#Feature dictionary used to initialise the transformer\n",
    "\n",
    "        with open(self.features_path, 'w') as f:\n",
    "            json.dump(features_dict, f, indent=2)\n",
    "\n",
    "        df = df.with_columns([\n",
    "            pl.col(col).fill_null('missing') for col in categorical_columns\n",
    "        ])#Ensures all missing values are represented the same way\n",
    "\n",
    "        df.write_csv(self.cleaned_csv_path)\n",
    "\n",
    "    def get_data_by_date_range(self, start_date: datetime, end_date: datetime, date_column: str = 'issue_d', return_unlisted_columns: bool = False):\n",
    "        '''\n",
    "        Extracts the rows from the cleaned dataset that fall within a specified date range.\n",
    "\n",
    "        All columns can be returned or only those used for modelling.\n",
    "\n",
    "        Args:\n",
    "            start_date (datetime): The start date (inclusive).\n",
    "            end_date (datetime): The end date (inclusive).\n",
    "            date_column (str): The column to filter by date.\n",
    "            return_unlisted_columns (bool): If True, return all columns; otherwise, drop datetime columns.\n",
    "\n",
    "        Returns:\n",
    "            pl.DataFrame: The filtered DataFrame.\n",
    "        '''\n",
    "        lf = pl.scan_csv(\n",
    "            self.cleaned_csv_path, \n",
    "            low_memory=True,\n",
    "            try_parse_dates=True)#Lazy frame is used for efficiency\n",
    "        \n",
    "        filtered_lf = lf.filter(\n",
    "            pl.col(date_column).is_between(start_date, end_date)\n",
    "        )\n",
    "\n",
    "        if return_unlisted_columns:\n",
    "            return filtered_lf.collect()\n",
    "        else:\n",
    "            df = filtered_lf.collect()\n",
    "            df = filtered_lf.collect()\n",
    "\n",
    "            datetime_columns = [column for column in df.columns if df[column].dtype == pl.Datetime]\n",
    "\n",
    "            return df.drop(datetime_columns)#Dates are stored both through their unicode representation and as a datetime, the datetime is only required for filtering, it's disgarded for modelling\n",
    "\n",
    "    def get_train_data(self, start_date: datetime, end_date: datetime)-> tuple[np.ndarray, np.ndarray]:\n",
    "        '''\n",
    "        Returns transformed training data for the autoencoder.\n",
    "        Extracts data within the specified date range, applies feature transformations, and returns the data along with a mask indicating non-missing values.\n",
    "\n",
    "        Args:\n",
    "            start_date (datetime): The start date for training data.\n",
    "            end_date (datetime): The end date for training data.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (transformed_data, missing_mask)\n",
    "                transformed_data (np.ndarray): The transformed feature matrix.\n",
    "                missing_mask (np.ndarray): Boolean mask where True indicates non-missing values.\n",
    "        '''\n",
    "        self.training_data_start_date = start_date\n",
    "        self.training_data_end_date = end_date\n",
    "\n",
    "        raw_data = self.get_data_by_date_range(start_date, end_date, date_column='issue_d', return_unlisted_columns=False)\n",
    "        transformed_data = self.transformer.fit_transform(raw_data)\n",
    "        \n",
    "        missing_mask = ~np.isnan(transformed_data)\n",
    "        transformed_data[~missing_mask] = 0.0\n",
    "\n",
    "        return transformed_data, missing_mask\n",
    "\n",
    "    def get_test_data(self, start_date: datetime, end_date: datetime)-> tuple[np.ndarray, np.ndarray]:\n",
    "        '''\n",
    "        Returns transformed test data for the autoencoder.\n",
    "\n",
    "        Extracts data within the specified date range, applies the previously fitted feature transformations, and returns the data and mask.\n",
    "        Ensures there is no overlap with the training data.\n",
    "\n",
    "        Args:\n",
    "            start_date (datetime): The start date for test data.\n",
    "            end_date (datetime): The end date for test data.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (transformed_data, not_null_mask)\n",
    "                transformed_data (np.ndarray): The transformed feature matrix.\n",
    "                not_null_mask (np.ndarray): Boolean mask where True indicates non-missing values.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If training data has not been set or if there is overlap with training data.\n",
    "        '''\n",
    "\n",
    "        if self.training_data_start_date is None or self.training_data_end_date is None:\n",
    "            raise ValueError('Training data not set. Please call get_training_data first.')\n",
    "        \n",
    "        if end_date < self.training_data_start_date or start_date > self.training_data_end_date:\n",
    "            raw_data = self.get_data_by_date_range(start_date, end_date, date_column='issue_d', return_unlisted_columns=False)\n",
    "            transformed_data = self.transformer.transform(raw_data)\n",
    "        \n",
    "            not_null_mask = ~np.isnan(transformed_data)\n",
    "            transformed_data[~not_null_mask] = 0.0\n",
    "\n",
    "            return transformed_data, not_null_mask\n",
    "            \n",
    "        else:\n",
    "            raise ValueError('There is an overlap between the training and test data.')\n",
    "        \n",
    "    def get_transformed_data_feature_names(self):\n",
    "        '''\n",
    "        Returns the names of the features after the column transformation is applied.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Array of feature names as output by the column transformer.\n",
    "        '''\n",
    "        return self.transformer.get_feature_names_out()\n",
    "    \n",
    "    def get_sigmoid_feature_mask(self, as_torch=False):\n",
    "        '''\n",
    "        Returns a mask indicating which features should use a sigmoid activation (i.e., one-hot or min-max scaled).\n",
    "\n",
    "        Args:\n",
    "            as_torch (bool): If True, return as a torch tensor; otherwise, as a numpy array.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray or torch.Tensor: Boolean mask for sigmoid features.\n",
    "        '''\n",
    "        mask = np.array([1 if ('one_hot_encoder' in name or 'min_max_scaler' in name) else 0\n",
    "                         for name in self.transformer.get_feature_names_out()])\n",
    "        \n",
    "        if as_torch:\n",
    "            return torch.tensor(mask, dtype=torch.bool)\n",
    "        else:\n",
    "            return mask\n",
    "    \n",
    "    def get_binary_feature_mask(self, as_torch=False):\n",
    "        '''\n",
    "        Returns a mask indicating which features are binary (i.e., one-hot encoded or flag features).\n",
    "\n",
    "        Args:\n",
    "            as_torch (bool): If True, return as a torch tensor; otherwise, as a numpy array.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray or torch.Tensor: Boolean mask for binary features.\n",
    "        '''\n",
    "        mask = np.array([1 if ('one_hot_encoder' in name or 'flag' in name) else 0\n",
    "                         for name in self.transformer.get_feature_names_out()])\n",
    "        if as_torch:\n",
    "            return torch.tensor(mask,dtype=torch.bool)\n",
    "        else:\n",
    "            return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def to_torch_dataloader(data, not_null_mask, batch_size: int = 64):\n",
    "    '''\n",
    "    Takes numpy arrays for the data and the corresponding not-null mask, converts them to PyTorch tensors,\n",
    "    and returns a DataLoader for batching and shuffling during model training.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): The feature matrix to be loaded.\n",
    "        not_null_mask (np.ndarray): Boolean mask indicating non-missing values in the data.\n",
    "        batch_size (int, optional): Number of samples per batch. Defaults to 64.\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: A PyTorch DataLoader yielding batches of (data, mask) tuples.\n",
    "    '''\n",
    "    data_tensor = torch.FloatTensor(data)\n",
    "    not_null_mask_tensor = torch.BoolTensor(not_null_mask)\n",
    "\n",
    "    # Create a dataset\n",
    "    dataset = TensorDataset(data_tensor, not_null_mask_tensor)\n",
    "\n",
    "    # Create dataloaders\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|test\n",
    "\n",
    "data_handler = DataHandler(csv_path='../local_data/all_lending_club_loan_data_2007-2018.csv')\n",
    "\n",
    "start = datetime(2010,1,1)\n",
    "end = datetime(2010,12,31)\n",
    "train_data, train_mask = data_handler.get_train_data(start, end)\n",
    "\n",
    "start = datetime(2011,1,1)\n",
    "end = datetime(2011,12,31)\n",
    "test_data, test_mask = data_handler.get_test_data(start, end)\n",
    "\n",
    "if (train_data.shape[1] != test_data.shape[1]):\n",
    "    raise ValueError('Training and testing data have different numbers of features')#Checks the same transformation is being applied to both datasets\n",
    "\n",
    "#Make into torch dataloader\n",
    "data_loader = to_torch_dataloader(train_data, train_mask)\n",
    "\n",
    "if type(data_loader) != DataLoader:\n",
    "    raise ValueError('Data loader is not a DataLoader')#Checks data is being correctly stored as a dataloader\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
