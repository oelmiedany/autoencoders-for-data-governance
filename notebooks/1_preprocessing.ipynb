{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp preprocessing\n",
    "#|export\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class DataHandler():\n",
    "    def __init__(self, csv_path: str = '../local_data/all_lending_club_loan_data_2007-2018.csv'):\n",
    "        self.cleaned_csv_path = f'{csv_path[:-4]}_cleaned.csv'\n",
    "        self.features_path = f'{csv_path[:-4]}_cleaned_features.json'\n",
    "\n",
    "        if not os.path.exists(self.cleaned_csv_path):\n",
    "            self.strip_non_data_rows_from_lending_club_data(csv_path)\n",
    "            self.clean_lending_club_data()\n",
    "\n",
    "        with open(self.features_path, 'r') as f:\n",
    "            self.features = json.load(f)\n",
    "\n",
    "        self.transformer = self.column_transformer = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('numeric', MinMaxScaler(), self.features['numeric']),\n",
    "                ('categorical', OneHotEncoder(drop=None), self.features['categorical'] )\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        self.training_data_start_date = None\n",
    "        self.training_data_end_date = None\n",
    "    \n",
    "    def strip_non_data_rows_from_lending_club_data(self, csv_path: str):\n",
    "        with open(csv_path, 'r') as f:\n",
    "            lines = [line for line in f if line[0].isdigit() or line.startswith('id')]\n",
    "\n",
    "        with open('temp.csv', 'w') as f:\n",
    "            f.writelines(lines)\n",
    "\n",
    "    def is_date_value(self, value: str) -> bool:\n",
    "        '''\n",
    "        Check if a value matches month-year format (e.g., 'sep-2015')\n",
    "        '''\n",
    "        if not isinstance(value, str) or '-' not in value:\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            month_str, year_str = value.lower().split('-')\n",
    "            return (month_str.title() in calendar.month_abbr and \n",
    "                    year_str.isdigit() and \n",
    "                    len(year_str) == 4)\n",
    "        except:\n",
    "            return False\n",
    "        \n",
    "    def parse_date_value(self, value: str) -> datetime:\n",
    "            '''\n",
    "            Convert month-year string to datetime\n",
    "            '''\n",
    "            if not isinstance(value, str):\n",
    "                return None\n",
    "            \n",
    "            try:\n",
    "                month_str, year_str = value.lower().split('-')\n",
    "                month_num = list(calendar.month_abbr).index(month_str.title())\n",
    "                return datetime(int(year_str), month_num, 1)\n",
    "            except:\n",
    "                return None\n",
    "            \n",
    "    def drop_undesired_columns(self, df: pl.DataFrame)-> pl.DataFrame:\n",
    "        explicit_columns_to_drop = [\n",
    "            'id',                       # Unique identifier\n",
    "            'funded_amnt',              # Redundant due to loan_amnt\n",
    "            'funded_amnt_inv',          # Redundant due to loan_amnt\n",
    "            'sub_grade',                # Redundant due to grade column\n",
    "            'emp_title',                # Too random\n",
    "            'title',                    # Redundant in relation to purpose column\n",
    "            'desc',                     # Mostly null\n",
    "            'url',                      # No predictive value\n",
    "            'mths_since_last_delinq',   # Mostly null and redundant due to delinq_2yrs column\n",
    "            'mths_since_last_record',   # Mostly null\n",
    "            'pymnt_plan',               # Always 'n'\n",
    "            'addr_state',               # Reduce dimensionality of this excerise\n",
    "            'zip_code'                  # Reduce dimensionality of this excerise\n",
    "        ]\n",
    "\n",
    "        # Dropping all secondary applicant, hardship, settlement, and joint columns as they are all null\n",
    "        implicit_columns_to_drop = (\n",
    "            'sec_app_',\n",
    "            'hardship_',\n",
    "            'settlement_',\n",
    "            'joint_',\n",
    "        )\n",
    "\n",
    "        implicit_columns_to_drop = [column for column in df.columns if column.startswith(implicit_columns_to_drop)]\n",
    "        high_null_cols = [column for column in df.columns  if (df[column].null_count() / len(df)) > 0.2]\n",
    "\n",
    "        columns_to_drop = explicit_columns_to_drop + implicit_columns_to_drop + high_null_cols\n",
    "        columns_to_drop = list(set(columns_to_drop))\n",
    "        \n",
    "        return df.drop(columns_to_drop)\n",
    "\n",
    "    def convert_employment_length(self, value: str) -> float:\n",
    "        if value is None or value == 'n/a':\n",
    "            return None\n",
    "        if value == '< 1 year':\n",
    "            return 0.5\n",
    "        if value == '10+ years':\n",
    "            return 10.0\n",
    "        return float(value.split()[0])\n",
    "\n",
    "    def clean_lending_club_data(self):\n",
    "        '''\n",
    "        Process Lending Club data:\n",
    "        1. Identify null and date (month-year format) columns\n",
    "        2. Deelete null columns\n",
    "        2. Convert identified date columns to datetime\n",
    "        '''\n",
    "\n",
    "        df = pl.read_csv('temp.csv')\n",
    "\n",
    "        null_columns = []\n",
    "        date_columns = []\n",
    "        \n",
    "        #Identify null and date columns\n",
    "        for col in df.columns:\n",
    "            # Check if column is all null\n",
    "            if df[col].is_null().all():\n",
    "                null_columns.append(col)\n",
    "                continue\n",
    "            \n",
    "            #Identify date columns\n",
    "            sample = df[col].drop_nulls().sample(1)\n",
    "            if self.is_date_value(str(sample[0])):\n",
    "                date_columns.append(col)\n",
    "\n",
    "        df = df.drop(null_columns)\n",
    "\n",
    "        #Convert date columns to datetime\n",
    "        if date_columns:\n",
    "            df = df.with_columns([\n",
    "                pl.col(col)\n",
    "                .str.to_lowercase()\n",
    "                .map_elements(self.parse_date_value, return_dtype=datetime)\n",
    "                .alias(col)\n",
    "                for col in date_columns\n",
    "            ])\n",
    "\n",
    "            #Include month and year columns for each date column\n",
    "            expressions = []\n",
    "            for col in date_columns:\n",
    "                expressions.extend([\n",
    "                    pl.col(col).dt.month().alias(f'{col}_month'),\n",
    "                    pl.col(col).dt.year().alias(f'{col}_year')\n",
    "                ])\n",
    "            df = df.with_columns(expressions)\n",
    "\n",
    "        df = df.with_columns([\n",
    "            pl.col('term').str.extract(r'(\\d+)').cast(pl.Int64).alias('term_months'),\n",
    "            pl.col('emp_length').map_elements(self.convert_employment_length, return_dtype=float).alias('employment_years'),\n",
    "            pl.col('grade').str.to_uppercase().map_elements(lambda x: ord(x) - 64, return_dtype=pl.Int64).alias('grade'),\n",
    "            pl.col('debt_settlement_flag').map_elements(lambda x: 1 if x == 'Y' else 0, return_dtype=pl.Int64).alias('debt_settlement_flag'),\n",
    "            pl.col('orig_projected_additional_accrued_interest').cast(pl.Float64).alias('orig_projected_additional_accrued_interest')\n",
    "            ]).drop(['emp_length', 'term'])\n",
    "        \n",
    "        df = self.drop_undesired_columns(df)\n",
    "\n",
    "        os.remove('temp.csv')\n",
    "\n",
    "        numeric_columns = [column for column, dtype in df.schema.items() \n",
    "                           if dtype in (pl.Float64, pl.Int64)]\n",
    "        categorical_columns = [col for col, dtype in df.schema.items() \n",
    "                               if dtype == pl.Utf8]\n",
    "\n",
    "        features_dict = {\n",
    "            'numeric': numeric_columns,\n",
    "            'categorical': categorical_columns\n",
    "        }\n",
    "\n",
    "        with open(self.features_path, 'w') as f:\n",
    "            json.dump(features_dict, f, indent=2)\n",
    "\n",
    "        df = df.with_columns([\n",
    "            pl.col(col).fill_null('missing') for col in categorical_columns\n",
    "        ])\n",
    "\n",
    "        df.write_csv(self.cleaned_csv_path)\n",
    "\n",
    "    def get_data_by_date_range(self, start_date: datetime, end_date: datetime, date_column: str = 'issue_d', return_unlisted_columns: bool = False):\n",
    "        '''\n",
    "        Extract rows between two datetime values using a lazy frame\n",
    "        '''\n",
    "        lf = pl.scan_csv(\n",
    "            self.cleaned_csv_path, \n",
    "            low_memory=True,\n",
    "            try_parse_dates=True)\n",
    "        \n",
    "        filtered_lf = lf.filter(\n",
    "            pl.col(date_column).is_between(start_date, end_date)\n",
    "        )\n",
    "\n",
    "        if return_unlisted_columns:\n",
    "            return filtered_lf.collect()\n",
    "        else:\n",
    "            df = filtered_lf.collect()\n",
    "            df = filtered_lf.collect()\n",
    "\n",
    "            datetime_columns = [column for column in df.columns if df[column].dtype == pl.Datetime]\n",
    "\n",
    "            return df.drop(datetime_columns)\n",
    "\n",
    "    def get_training_data(self, start_date: datetime, end_date: datetime)-> tuple[np.ndarray, np.ndarray]:\n",
    "        '''Get transformed data ready for the autoencoder'''\n",
    "        self.training_data_start_date = start_date\n",
    "        self.training_data_end_date = end_date\n",
    "\n",
    "        raw_data = self.get_data_by_date_range(start_date, end_date, date_column='issue_d', return_unlisted_columns=False)\n",
    "        transformed_data = self.transformer.fit_transform(raw_data)\n",
    "        \n",
    "        missing_mask = np.isnan(transformed_data)\n",
    "        transformed_data[missing_mask] = -1\n",
    "\n",
    "        return transformed_data, missing_mask\n",
    "\n",
    "    def get_test_data(self, start_date: datetime, end_date: datetime)-> tuple[np.ndarray, np.ndarray]:\n",
    "        if self.training_data_start_date is None or self.training_data_end_date is None:\n",
    "            raise ValueError('Training data not set. Please call get_training_data first.')\n",
    "        \n",
    "        if end_date < self.training_data_start_date or start_date > self.training_data_end_date:\n",
    "            raw_data = self.get_data_by_date_range(start_date, end_date, date_column='issue_d', return_unlisted_columns=False)\n",
    "            transformed_data = self.transformer.transform(raw_data)\n",
    "        \n",
    "            missing_mask = np.isnan(transformed_data)\n",
    "            transformed_data[missing_mask] = -1\n",
    "\n",
    "            return transformed_data, missing_mask\n",
    "            \n",
    "        else:\n",
    "            raise ValueError('There is an overlap between the training and test data.')\n",
    "        \n",
    "    def get_transformed_data_feature_names(self):\n",
    "        return self.transformer.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|test\n",
    "\n",
    "data_handler = DataHandler(csv_path='../local_data/all_lending_club_loan_data_2007-2018.csv')\n",
    "\n",
    "start = datetime(2010,1,1)\n",
    "end = datetime(2015,12,1)\n",
    "training_data, missing_mask = #|export.get_training_data(start, end)\n",
    "\n",
    "if len(training_data[0]) != 108:\n",
    "    raise ValueError()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
