{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp autoencoders\n",
    "#|export\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size:int, sigmoid_mask: torch.Tensor):\n",
    "        '''\n",
    "        Variational Autoencoder for data compression and reconstruction\n",
    "        \n",
    "        Args:\n",
    "            input_size (int): Dimension of input features\n",
    "            hidden_size (int): Dimension of hidden layers (default: 64)\n",
    "            latent_size (int): Dimension of latent space (default: 32)\n",
    "        '''\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "\n",
    "        #Stores key model parameters\n",
    "        self.input_size = input_size\n",
    "\n",
    "        hidden_size_1 = 64\n",
    "        hidden_size_2 = 32\n",
    "        latent_size = 16\n",
    "        \n",
    "        # Encoder architecture\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size_1),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(hidden_size_1, hidden_size_2), \n",
    "            nn.SELU(),\n",
    "        )\n",
    "        \n",
    "        # Latent space parameters\n",
    "        self.fc_mean = nn.Linear(hidden_size_2, latent_size)  # Mean of latent distribution\n",
    "        self.fc_log_variance = nn.Linear(hidden_size_2, latent_size)  # Log variance of latent distribution\n",
    "        \n",
    "        # Decoder architecture\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_size, hidden_size_2),\n",
    "            nn.SELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size_2, hidden_size_1),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(hidden_size_1, input_size)\n",
    "        )\n",
    "\n",
    "        self.register_buffer('sigmoid_mask', sigmoid_mask.unsqueeze(0))\n",
    "        \n",
    "    def encode(self, x:torch.Tensor)->tuple[torch.Tensor, torch.Tensor]:\n",
    "        '''\n",
    "        Encode input data into latent space parameters\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (mean, log_variance) parameters of the latent distribution\n",
    "        '''\n",
    "        # Generate latent space parameters\n",
    "        hidden = self.encoder(x)\n",
    "        mean = self.fc_mean(hidden)\n",
    "        log_variance = self.fc_log_variance(hidden)\n",
    "        return mean, log_variance\n",
    "    \n",
    "    def reparameterise(self, mean:torch.Tensor, log_variance:torch.Tensor)->torch.Tensor:\n",
    "        '''\n",
    "        Reparameterization to enable backpropagation through random sampling\n",
    "        \n",
    "        Args:\n",
    "            mean (torch.Tensor): Mean of the latent distribution\n",
    "            log_variance (torch.Tensor): Log variance of the latent distribution\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Sampled point from the latent distribution\n",
    "        '''\n",
    "        log_variance = F.softplus(log_variance) + 1e-6 \n",
    "\n",
    "        std = torch.exp(0.5 * log_variance)\n",
    "        eps = torch.randn_like(std)  # Random noise from standard normal\n",
    "        return mean + eps * std\n",
    "    \n",
    "    def decode(self, latent_vector:torch.Tensor)->torch.Tensor:\n",
    "        '''\n",
    "        Decode latent representation back to input space\n",
    "        \n",
    "        Args:\n",
    "            latent_vector (torch.Tensor): Latent space representation\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Reconstructed input\n",
    "        '''\n",
    "        return self.decoder(latent_vector)\n",
    "    \n",
    "    def forward(self, x:torch.Tensor)->tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        '''\n",
    "        Forward pass through the VAE\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (reconstruction, mean)\n",
    "        '''\n",
    "        mean, _ = self.encode(x)\n",
    "        #latent_vector = self.reparameterise(mean, log_variance)\n",
    "        raw_reconstruction = self.decode(mean)\n",
    "\n",
    "        reconstruction = torch.where(\n",
    "            self.sigmoid_mask,                     # broadcast to [B,input_size]\n",
    "            torch.sigmoid(raw_reconstruction),\n",
    "            raw_reconstruction\n",
    "            )\n",
    "\n",
    "        return reconstruction, mean\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
