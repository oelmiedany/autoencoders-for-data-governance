{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp autoencoders\n",
    "#|export\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size:int, sigmoid_mask: torch.Tensor):\n",
    "        '''\n",
    "        Variational Autoencoder for data compression and reconstruction.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): The number of input features.\n",
    "            sigmoid_mask (torch.Tensor): Boolean mask indicating which input features should have a sigmoid activation applied in the output layer (e.g., for binary or categorical features). Should be a 1D tensor of length input_size.\n",
    "        '''\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "\n",
    "        #Stores key model parameters\n",
    "        self.input_size = input_size\n",
    "\n",
    "        hidden_size_1 = 64\n",
    "        hidden_size_2 = 32\n",
    "        latent_size = 16\n",
    "        \n",
    "        # Encoder architecture\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size_1),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(hidden_size_1, hidden_size_2), \n",
    "            nn.SELU(),\n",
    "        )\n",
    "        \n",
    "        # Latent space parameters\n",
    "        self.fc_mean = nn.Linear(hidden_size_2, latent_size)  # Mean of latent distribution\n",
    "        self.fc_log_variance = nn.Linear(hidden_size_2, latent_size)  # Log variance of latent distribution\n",
    "        \n",
    "        # Decoder architecture\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_size, hidden_size_2),\n",
    "            nn.SELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size_2, hidden_size_1),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(hidden_size_1, input_size)\n",
    "        )\n",
    "\n",
    "        self.register_buffer('sigmoid_mask', sigmoid_mask.unsqueeze(0))#Only values between 0 and 1 have a sigmoid function applied to them at the end\n",
    "        \n",
    "    def encode(self, x:torch.Tensor)->tuple[torch.Tensor, torch.Tensor]:\n",
    "        '''\n",
    "        Encodes the input data into the parameters of the latent space distribution (mean and log-variance).\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [batch_size, input_size].\n",
    "\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, torch.Tensor]:\n",
    "                - mean (torch.Tensor): Mean of the latent Gaussian distribution, shape [batch_size, latent_size].\n",
    "                - log_variance (torch.Tensor): Log-variance of the latent Gaussian distribution, shape [batch_size, latent_size].\n",
    "        '''\n",
    "\n",
    "        # Generate latent space parameters\n",
    "        hidden = self.encoder(x)\n",
    "        mean = self.fc_mean(hidden)\n",
    "        log_variance = self.fc_log_variance(hidden)\n",
    "        return mean, log_variance\n",
    "    \n",
    "    def reparameterise(self, mean:torch.Tensor, log_variance:torch.Tensor)->torch.Tensor:\n",
    "        '''\n",
    "        Applies the reparameterisation to allow sampling from the latent space, enabling backpropagation through stochastic nodes.\n",
    "\n",
    "        Args:\n",
    "            mean (torch.Tensor): Mean of the latent Gaussian distribution, shape [batch_size, latent_size].\n",
    "            log_variance (torch.Tensor): Log-variance of the latent Gaussian distribution, shape [batch_size, latent_size].\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Sampled latent vector, shape [batch_size, latent_size].\n",
    "        '''\n",
    "        log_variance = F.softplus(log_variance) + 1e-6 \n",
    "\n",
    "        std = torch.exp(0.5 * log_variance)\n",
    "        eps = torch.randn_like(std)  # Random noise from standard normal\n",
    "        return mean + eps * std\n",
    "    \n",
    "    def decode(self, latent_vector:torch.Tensor)->torch.Tensor:\n",
    "        '''\n",
    "        Decodes a latent vector back into the input feature space.\n",
    "\n",
    "        Args:\n",
    "            latent_vector (torch.Tensor): Latent space representation, shape [batch_size, latent_size].\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Reconstructed input, shape [batch_size, input_size].\n",
    "        '''\n",
    "        return self.decoder(latent_vector)\n",
    "    \n",
    "    def forward(self, x:torch.Tensor)->tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        '''\n",
    "        Runs a forward pass through the VAE:\n",
    "        \n",
    "        - Encodes the input\n",
    "        - Decodes from the latent mean\n",
    "        - Applies the appropriate output activations.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [batch_size, input_size].\n",
    "\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, torch.Tensor]:\n",
    "                - reconstruction (torch.Tensor): The reconstructed input, with sigmoid applied to features as specified by sigmoid_mask.\n",
    "                - mean (torch.Tensor): The mean of the latent distribution for each input in the batch.\n",
    "        '''\n",
    "        mean, _ = self.encode(x)\n",
    "        #latent_vector = self.reparameterise(mean, log_variance)#Reparmatisation dropped to avoid latent space collapse\n",
    "        raw_reconstruction = self.decode(mean)\n",
    "\n",
    "        reconstruction = torch.where(\n",
    "            self.sigmoid_mask,\n",
    "            torch.sigmoid(raw_reconstruction),\n",
    "            raw_reconstruction\n",
    "            )\n",
    "\n",
    "        return reconstruction, mean\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
